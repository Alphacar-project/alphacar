import { Injectable, OnModuleInit } from '@nestjs/common';
import { ConfigService } from '@nestjs/config';
import {
  BedrockRuntimeClient,
  ConverseCommand,
  ConverseCommandInput
} from '@aws-sdk/client-bedrock-runtime';
import { BedrockEmbeddings } from '@langchain/aws';
import { FaissStore } from '@langchain/community/vectorstores/faiss';
import { Document } from '@langchain/core/documents';
import * as fs from 'fs';

@Injectable()
export class ChatService implements OnModuleInit {
  private embeddings: BedrockEmbeddings;
  private vectorStore: FaissStore;
  private bedrockClient: BedrockRuntimeClient;
  private readonly VECTOR_STORE_PATH = './vector_store';

  constructor(private configService: ConfigService) {}

  async onModuleInit() {
    const accessKeyId = this.configService.get<string>('AWS_ACCESS_KEY_ID') ?? '';
    const secretAccessKey = this.configService.get<string>('AWS_SECRET_ACCESS_KEY') ?? '';
    const region = this.configService.get<string>('AWS_REGION') ?? 'us-east-1';

    // 1. ì„ë² ë”© ëª¨ë¸ (LangChain)
    this.embeddings = new BedrockEmbeddings({
      region: region,
      credentials: { accessKeyId, secretAccessKey },
      model: 'amazon.titan-embed-text-v2:0',
    });

    // 2. Bedrock SDK Client (Converse APIìš©)
    this.bedrockClient = new BedrockRuntimeClient({
      region: region,
      credentials: { accessKeyId, secretAccessKey },
    });

    await this.loadVectorStore();
  }

  private async loadVectorStore() {
    if (fs.existsSync(this.VECTOR_STORE_PATH)) {
      console.log('ğŸ“‚ Loading existing vector store...');
      this.vectorStore = await FaissStore.load(this.VECTOR_STORE_PATH, this.embeddings);
    } else {
      console.log('ğŸ†• Creating new vector store...');
      this.vectorStore = await FaissStore.fromDocuments(
        [new Document({ pageContent: 'Init Data', metadata: { source: 'init' } })],
        this.embeddings
      );
      await this.vectorStore.save(this.VECTOR_STORE_PATH);
    }
  }

  async addKnowledge(content: string, source: string) {
    const doc = new Document({ pageContent: content, metadata: { source } });
    await this.vectorStore.addDocuments([doc]);
    await this.vectorStore.save(this.VECTOR_STORE_PATH);
    return { message: 'Knowledge added.', source };
  }

  // [ê¸°ì¡´ ìœ ì§€] AI í…ìŠ¤íŠ¸ ê¸°ë°˜ ì°¨ì¢… ë¶„ë¥˜ (Llama 3.3 70B)
  async classifyCar(modelName: string): Promise<string> {
    const prompt = `Classify '${modelName}' into ONE: [Sedan, SUV, Truck, Van, Light Car, Sports Car, Hatchback]. No explanation.`;
    const input: ConverseCommandInput = {
      modelId: 'us.meta.llama3-3-70b-instruct-v1:0',
      messages: [{ role: 'user', content: [{ text: prompt }] }],
      inferenceConfig: { maxTokens: 10, temperature: 0 },
    };
    try {
      const command = new ConverseCommand(input);
      const res = await this.bedrockClient.send(command);
      return res.output?.message?.content?.[0]?.text?.trim().split(/[\n,.]/)[0].trim() || 'ê¸°íƒ€';
    } catch (e) { return 'ê¸°íƒ€'; }
  }

  // =================================================================================
  // [ì‹ ê·œ ê¸°ëŠ¥] ì´ë¯¸ì§€ ì±„íŒ… (Llama 3.2 Vision ì ìš©)
  // =================================================================================

  async chatWithImage(imageBuffer: Buffer, mimeType: string = 'image/jpeg') {
    console.log("ğŸ“¸ Image received, analyzing with Llama 3.2 Vision...");

    const carModelName = await this.identifyCarWithLlama(imageBuffer, mimeType);

    if (carModelName === 'NOT_CAR') {
        return {
            response: "ì£„ì†¡í•©ë‹ˆë‹¤. ì‚¬ì§„ì—ì„œ ìë™ì°¨ë¥¼ ëª…í™•í•˜ê²Œ ì‹ë³„í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì°¨ëŸ‰ì´ ì˜ ë³´ì´ëŠ” ì‚¬ì§„ìœ¼ë¡œ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”.",
            context_used: [],
            identified_car: null
        };
    }

    console.log(`ğŸ“¸ Identified Car: ${carModelName}`);

    const userPrompt = `${carModelName} ëª¨ë¸ì˜ ê°€ê²©ê³¼ ì£¼ìš” íŠ¹ì§•ì— ëŒ€í•´ ìƒì„¸íˆ ì•Œë ¤ì¤˜.`;

    const chatResult = await this.chat(userPrompt);

    return {
        ...chatResult,
        identified_car: carModelName
    };
  }

  private async identifyCarWithLlama(imageBuffer: Buffer, mimeType: string): Promise<string> {
    const modelId = 'us.meta.llama3-2-90b-instruct-v1:0';

    const prompt = `
    ì´ë¯¸ì§€ì— ìˆëŠ” ì°¨ëŸ‰ì„ ë³´ê³  ë‹¤ìŒ ì„¸ ê°€ì§€ ì§€ì¹¨ì— ë”°ë¼ ì‘ë‹µí•´.
    1. ì´ë¯¸ì§€ ì† ìë™ì°¨ì˜ ì œì¡°ì‚¬ëª…ê³¼ **ì •í™•í•œ ëª¨ë¸ëª…**(ì˜ˆ: "í˜„ëŒ€ ê·¸ëœì €", "ê¸°ì•„ ì˜ë Œí† ", "ì œë„¤ì‹œìŠ¤ G80")ì„ ì‹ë³„í•´.
    2. **ì‘ë‹µì€ ì˜¤ì§** ì‹ë³„ëœ ëª¨ë¸ëª… **í•˜ë‚˜**ë§Œ **í•œêµ­ì–´(í•œê¸€)**ë¡œ ì¶œë ¥í•´. ë‹¤ë¥¸ ì„¤ëª…ì´ë‚˜ ë¬¸ì¥ì€ **ì ˆëŒ€** í¬í•¨í•˜ì§€ ë§ˆ.
    3. ì´ë¯¸ì§€ì— ìë™ì°¨ê°€ ì—†ê±°ë‚˜ ì‹ë³„í•  ìˆ˜ ì—†ë‹¤ë©´, **ì˜¤ì§** "**NOT_CAR**"ë¼ëŠ” í…ìŠ¤íŠ¸ë§Œ ì¶œë ¥í•´.
    `;

    const format = mimeType === 'image/png' ? 'png' :
                   mimeType === 'image/webp' ? 'webp' :
                   mimeType === 'image/gif' ? 'gif' : 'jpeg';

    const input: ConverseCommandInput = {
      modelId: modelId,
      messages: [
        {
          role: 'user',
          content: [
            {
              image: {
                format: format,
                source: {
                  bytes: imageBuffer,
                },
              },
            },
            {
              text: prompt,
            },
          ],
        },
      ],
      inferenceConfig: { maxTokens: 100, temperature: 0.1 },
    };

    try {
      const command = new ConverseCommand(input);
      const response = await this.bedrockClient.send(command);

      let text = response.output?.message?.content?.[0]?.text?.trim() || 'NOT_CAR';

      text = text.replace(/\.$/, '').trim();

      if (text.includes('NOT_CAR')) return 'NOT_CAR';

      return text;
    } catch (e) {
      console.error("ğŸ”¥ Bedrock Vision Error:", e);
      return 'NOT_CAR';
    }
  }

  // =================================================================================

  async chat(userMessage: string) {
    // 1. RAG ê²€ìƒ‰ 
    // ê²€ìƒ‰ëŸ‰ì„ 50ê°œë¡œ ìœ ì§€í•©ë‹ˆë‹¤.
    let results = await this.vectorStore.similaritySearch(userMessage, 50); 

    const context = results.map((r) => r.pageContent).join('\n\n');
    const sources = results.map((r) => r.metadata.source);

    console.log(`ğŸ” Context Length: ${context.length} characters`);

    // ğŸ‘‡ [FIX: ë¹„êµ ëª¨ë“œ ê°ì§€ ë¡œì§] ì‚¬ìš©ìê°€ ë¹„êµë¥¼ ì›í•˜ëŠ”ì§€ ê°ì§€í•©ë‹ˆë‹¤.
    const comparisonKeywords = ['ë¹„êµ', 'ëŒ€ë¹„', 'ë­ê°€ ë”', 'ì°¨ì´'];
    const isComparisonQuery = comparisonKeywords.some(keyword => userMessage.includes(keyword)) && 
                              (userMessage.includes('ì˜ë‚˜íƒ€') && userMessage.includes('K5'));

    // 2. ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ë§í¬ ID ì¹˜í™˜ ë¡œì§ ê°•í™” ë° ì´ë¯¸ì§€ ì¶œë ¥ ê°•ì œ)
    let systemPrompt = `
    You are the AI Automotive Specialist for 'AlphaCar'.

    [CORE RULES - STRICT COMPLIANCE]
    1. **LANGUAGE**: Answer strictly in **Korean (Hangul)**. No Hanja.
    2. **GROUNDING**: Answer SOLELY based on the provided [Context].
    3. **GUARDRAIL**: If the user asks about Non-Automotive topics, REJECT immediately.

    [CONVERSATION FLOW - KEEP IT ALIVE]
    **Do NOT just answer and stop.** Always end your response with a **Follow-up Question** to guide the user.

    - **If you recommended cars**: "ì´ ì¤‘ì—ì„œ ë§ˆìŒì— ë“œëŠ” ëª¨ë¸ì´ ìˆìœ¼ì‹ ê°€ìš”? ì•„ë‹ˆë©´ ë‹¤ë¥¸ ì¡°ê±´(ì˜ˆ: ì—°ë¹„, ë””ìì¸)ìœ¼ë¡œ ë” ì°¾ì•„ë³¼ê¹Œìš”?"
    - **If you gave a price**: "ìƒê°í•˜ì‹  ì˜ˆì‚° ë²”ìœ„ì— ë§ìœ¼ì‹ ê°€ìš”? í• ë¶€ ê²¬ì ì´ë‚˜ ì˜µì…˜ ì •ë³´ë„ ì•Œë ¤ë“œë¦´ê¹Œìš”?"
    - **If info is missing**: "ë” ì •í™•í•œ ì¶”ì²œì„ ìœ„í•´ ì„ í˜¸í•˜ì‹œëŠ” ë¸Œëœë“œë‚˜ ì—°ë£Œ íƒ€ì…(ì „ê¸°/ê°€ì†”ë¦°)ì„ ì•Œë ¤ì£¼ì‹œê² ì–´ìš”?"
    - **General**: Act like a friendly and proactive car dealer.

    [RESPONSE_STRATEGY]
    1. **QUANTITY**: Recommend at least 3 different models if possible.
    2. **FORMAT**: Use a numbered list.
    
    // ğŸ‘‡ [ìµœì¢… FIX] ë¹„êµ ì¿¼ë¦¬ì¼ ê²½ìš°, êµ¬ì¡°í™”ëœ ë¸”ë¡ ì¶œë ¥ì„ ê°•ì œí•˜ì—¬ ì •ë³´ ëˆ„ë½ì„ ë§‰ìŠµë‹ˆë‹¤.
    ${isComparisonQuery ? `
    3. **COMPARISON_RULE (CRITICAL)**: The user wants a side-by-side comparison. YOU MUST NOT fail to find either model. Search the Context for both "ì˜ë‚˜íƒ€" and "K5". Your entire response MUST output two distinct, separate content blocks (one for Sonata, one for K5) separated only by TWO consecutive newlines (\\n\\n). 
    4. **BLOCK_STRUCTURE**: Each block MUST start with the image link for the model it describes, followed immediately by a short summary of its Price Range and Key Options text. DO NOT output a comparison table. DO NOT output the block numbers (1, 2).
    ` : `
    3. **IMAGE_PRIORITY**: If the context provides the ImageURL and BaseTrimId for the car you are discussing, you MUST include its image and link following the [IMAGE RENDERING & LINKING LOGIC].
    `}

    [SMART FILTERING LOGIC]
    1. **Price Flexibility**: Allow Â±10% margin.
    2. **Type Filtering**:
        - "Sedan" -> Sedan/Coupe/Hatchback.
        - "SUV" -> SUV/RV.
    3. **Scenarios**:
        - "Camping": SUV, Van.
        - "Commute/First Car": Compact Sedan, Hybrid, Light Car.

    [IMAGE RENDERING & LINKING LOGIC]
    - MUST display images if 'ImageURL' exists in context.
    - **CRITICAL**: You MUST wrap the image in a link to the quote page.

    - **â›” STRICT RULE (NO RAW URLs)**:
      - Do NOT write the raw Image URL (http://...) as plain text in the response.
      - ONLY output the URL inside the Markdown link syntax.
      
    - **ID Selection Rules (Smart Linking)**:
      1. Find the **BaseTrimId** value from the [ì‹œìŠ¤í…œ ë°ì´í„°] section of the vehicle you are describing.
      2. **ABSOLUTELY MUST**: The resulting link MUST use the actual ID value, not a placeholder.
      
    - **Link Format (Template - MUST FOLLOW)**:
      [![Car Model Name](ImageURL)](/quote/personal/result?trimId=ì‹¤ì œ_BaseTrimId_ê°’)

    [Context]
    ${context}
    `;

    // 3. Bedrock Converse API (Llama 3.3 70B - í…ìŠ¤íŠ¸ ìƒì„±ìš©)
    const guardrailId = this.configService.get<string>('BEDROCK_GUARDRAIL_ID');
    const guardrailVersion = this.configService.get<string>('BEDROCK_GUARDRAIL_VERSION') || 'DRAFT';

    const input: ConverseCommandInput = {
      modelId: 'us.meta.llama3-3-70b-instruct-v1:0',
      messages: [{ role: 'user', content: [{ text: userMessage }] }],
      system: [{ text: systemPrompt }],
      inferenceConfig: { maxTokens: 2048, temperature: 0.2 },
    };

    if (guardrailId && guardrailId.length > 5) {
        input.guardrailConfig = {
            guardrailIdentifier: guardrailId,
            guardrailVersion: guardrailVersion,
            trace: 'enabled',
        };
        console.log(`ğŸ›¡ï¸ Guardrail Active: ${guardrailId} (${guardrailVersion})`);
    }

    try {
      const command = new ConverseCommand(input);
      const response = await this.bedrockClient.send(command);

      if (response.stopReason === 'guardrail_intervened') {
          console.log("ğŸš« Blocked by AWS Guardrail!");
          return {
              response: "ğŸš« [ìë™ ì°¨ë‹¨] ìë™ì°¨ì™€ ê´€ë ¨ ì—†ëŠ” ì§ˆë¬¸(ê¸ˆìœµ, ì •ì¹˜, ìš•ì„¤ ë“±)ì€ ë‹µë³€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
              context_used: [],
          };
      }

      const outputText = response.output?.message?.content?.[0]?.text || '';
      return { response: outputText, context_used: sources };

    } catch (e: any) {
      console.error("ğŸ”¥ AWS Bedrock Error:", e.message);
      if (e.name === 'ValidationException' && e.message.includes('guardrail')) {
         return {
             response: `âš ï¸ [System Error] Guardrail Config Error.\n${e.message}`,
             context_used: []
         };
      }
      return {
          response: "ì£„ì†¡í•©ë‹ˆë‹¤. AI ì„œë²„ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
          context_used: []
      };
    }
  }
}
